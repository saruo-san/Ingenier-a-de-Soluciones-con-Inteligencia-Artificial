{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Construcci√≥n de Agentes con LangChain\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender las ventajas de usar un framework como LangChain para el desarrollo de agentes.\n",
    "- Aprender a definir herramientas (Tools) de forma sencilla con el decorador `@tool`.\n",
    "- Crear un agente utilizando el constructor `create_openai_tools_agent`.\n",
    "- Ejecutar el agente y gestionar la interacci√≥n mediante el `AgentExecutor`.\n",
    "\n",
    "## ¬øPor qu√© usar LangChain?\n",
    "\n",
    "En los notebooks anteriores, construimos agentes desde cero. Primero, parseando texto, y luego, usando `function calling` nativo. Aunque el `function calling` es una gran mejora, todav√≠a ten√≠amos que:\n",
    "\n",
    "1.  **Gestionar el historial de mensajes (`messages`) manualmente**: A√±adir cada respuesta del usuario, del asistente y de la herramienta a la lista.\n",
    "2.  **Orquestar el flujo de llamadas**: Escribir la l√≥gica `if/else` para decidir si llamar a una herramienta, ejecutarla y volver a llamar al modelo.\n",
    "3.  **Formatear las herramientas**: Escribir el JSON Schema para cada herramienta, lo cual es propenso a errores.\n",
    "\n",
    "**LangChain** es un framework que abstrae toda esta complejidad. Act√∫a como una capa intermedia que simplifica enormemente la creaci√≥n de aplicaciones basadas en LLMs, incluyendo los agentes. \n",
    "\n",
    "**Ventajas clave:**\n",
    "- **Componentes Modulares**: Ofrece piezas reutilizables (LLMs, Prompts, Herramientas, etc.) que se pueden ensamblar f√°cilmente.\n",
    "- **Agentes Listos para Usar**: Proporciona abstracciones de alto nivel como el `AgentExecutor` que manejan el ciclo de razonamiento (ReAct) por nosotros.\n",
    "- **Integraciones**: Se conecta con cientos de fuentes de datos, APIs y otros servicios de forma nativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instalaci√≥n y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai openai wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM de LangChain configurado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wikipedia\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Configurar el idioma de Wikipedia\n",
    "wikipedia.set_lang('es')\n",
    "\n",
    "# --- Configuraci√≥n del LLM con LangChain ---\n",
    "# LangChain act√∫a como un \"envoltorio\" (wrapper) sobre el cliente de OpenAI\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        openai_api_base=os.environ.get(\"GITHUB_BASE_URL\"),\n",
    "        openai_api_key=os.environ.get(\"GITHUB_TOKEN\"),\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"‚úÖ LLM de LangChain configurado.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error configurando el LLM: {e}\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Definici√≥n de Herramientas con LangChain\n",
    "\n",
    "LangChain simplifica enormemente la creaci√≥n de herramientas. En lugar de escribir un JSON Schema manual, simplemente usamos el decorador `@tool` sobre una funci√≥n de Python. LangChain se encarga de inferir el esquema a partir de la firma de la funci√≥n y su docstring.\n",
    "\n",
    "El docstring es **muy importante**, ya que se usa como la descripci√≥n que el LLM ve para decidir si usar la herramienta o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Herramientas de LangChain definidas.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_wikipedia_summary(query: str) -> str:\n",
    "    \"\"\"Busca en Wikipedia un tema y devuelve un resumen de 2 frases. Es ideal para obtener informaci√≥n sobre personas, lugares o conceptos hist√≥ricos y cient√≠ficos.\"\"\"\n",
    "    try:\n",
    "        summary = wikipedia.summary(query, sentences=2)\n",
    "        return summary\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        return f\"No se encontr√≥ ninguna p√°gina para '{query}'.\"\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return f\"La b√∫squeda para '{query}' es ambigua. Opciones: {e.options[:3]}\"\n",
    "\n",
    "tools = [get_wikipedia_summary]\n",
    "\n",
    "print(\"‚úÖ Herramientas de LangChain definidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creaci√≥n del Agente\n",
    "\n",
    "Para crear el agente, necesitamos dos cosas:\n",
    "\n",
    "1.  **Un Prompt**: Una plantilla que le dice al agente c√≥mo razonar y c√≥mo usar las herramientas. LangChain ya tiene prompts pre-construidos y optimizados para esto. Usaremos `hub.pull(\"hwchase17/openai-functions-agent\")` para obtener una plantilla probada.\n",
    "2.  **El Agente en s√≠**: Usamos la funci√≥n `create_openai_tools_agent`, que une el LLM, las herramientas y el prompt.\n",
    "\n",
    "El resultado es un `Runnable` de LangChain, que es el agente listo para ser ejecutado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agente de LangChain creado.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "\n",
    "# Descargar un prompt pre-dise√±ado y optimizado para agentes con function calling\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "# Unir el LLM, las herramientas y el prompt para crear el agente\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "print(\"‚úÖ Agente de LangChain creado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ejecuci√≥n del Agente con `AgentExecutor`\n",
    "\n",
    "El `agent` que creamos en el paso anterior es solo el \"cerebro\". Sabe c√≥mo razonar, pero no puede ejecutar el ciclo de `Pensamiento -> Acci√≥n -> Observaci√≥n` por s√≠ mismo.\n",
    "\n",
    "Para eso, usamos el **`AgentExecutor`**. Esta clase toma el agente y las herramientas, y se encarga de toda la orquestaci√≥n:\n",
    "\n",
    "- Llama al agente con la entrada del usuario.\n",
    "- Si el agente decide usar una herramienta, el `AgentExecutor` la ejecuta.\n",
    "- Pasa el resultado de la herramienta de vuelta al agente.\n",
    "- Repite el proceso hasta que el agente da una respuesta final.\n",
    "- Gestiona el historial de la conversaci√≥n (`chat_history`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AgentExecutor listo para funcionar.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "print(\"‚úÖ AgentExecutor listo para funcionar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Invocando al Agente\n",
    "\n",
    "Ahora, simplemente llamamos al m√©todo `invoke` del `agent_executor` con la pregunta. El par√°metro `chat_history` es opcional pero √∫til para conversaciones de seguimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_wikipedia_summary` with `{'query': 'Marie Curie'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mMaria Salomea Sk≈Çodowska-Curie,[A]‚Äã[B]‚Äã m√°s conocida como Marie Curie[C]‚Äã[B]‚Äã o Madame Curie (Varsovia, 7 de noviembre de 1867-Passy, 4 de julio de 1934), fue una f√≠sica y qu√≠mica de origen polaco. Pionera en el campo de la radiactividad, es la primera y √∫nica persona en recibir dos premios Nobel en distintas especialidades cient√≠ficas: F√≠sica y Qu√≠mica.[D]‚Äã Tambi√©n fue la primera mujer en ocupar el puesto de profesora en la Universidad de Par√≠s y la primera en recibir sepultura con honores en el Pante√≥n de Par√≠s por m√©ritos propios en 1995.[E]‚Äã\n",
      "Naci√≥ en Varsovia, en lo que entonces era el Zarato de Polonia (territorio administrado por el Imperio ruso).\u001b[0m\u001b[32;1m\u001b[1;3mMarie Curie fue una f√≠sica y qu√≠mica polaca, pionera en el campo de la radiactividad, y la primera persona en recibir dos premios Nobel en distintas disciplinas cient√≠ficas: F√≠sica y Qu√≠mica. Entre sus logros m√°s importantes destacan el descubrimiento de los elementos radiactivos polonio y radio, as√≠ como su contribuci√≥n al desarrollo de aplicaciones m√©dicas de la radiactividad. Adem√°s, fue la primera mujer en ser profesora en la Universidad de Par√≠s y en recibir sepultura con honores en el Pante√≥n de Par√≠s por m√©ritos propios.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "üèÅ Respuesta Final del Agente: {'input': '¬øQui√©n fue Marie Curie y cu√°les fueron sus logros m√°s importantes?', 'chat_history': [], 'output': 'Marie Curie fue una f√≠sica y qu√≠mica polaca, pionera en el campo de la radiactividad, y la primera persona en recibir dos premios Nobel en distintas disciplinas cient√≠ficas: F√≠sica y Qu√≠mica. Entre sus logros m√°s importantes destacan el descubrimiento de los elementos radiactivos polonio y radio, as√≠ como su contribuci√≥n al desarrollo de aplicaciones m√©dicas de la radiactividad. Adem√°s, fue la primera mujer en ser profesora en la Universidad de Par√≠s y en recibir sepultura con honores en el Pante√≥n de Par√≠s por m√©ritos propios.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"¬øQui√©n fue Marie Curie y cu√°les fueron sus logros m√°s importantes?\"\n",
    "\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": query,\n",
    "    \"chat_history\": []\n",
    "})\n",
    "\n",
    "print(f\"üèÅ Respuesta Final del Agente: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Como hemos visto, LangChain reduce dr√°sticamente la cantidad de c√≥digo repetitivo y la complejidad de construir un agente. Nos hemos podido centrar en:\n",
    "\n",
    "1.  **Definir la l√≥gica de la herramienta**: La funci√≥n `get_wikipedia_summary`.\n",
    "2.  **Ensamblar los componentes**: Unir el LLM, las herramientas y un prompt usando las abstracciones de LangChain.\n",
    "\n",
    "El `AgentExecutor` se encarg√≥ de todo el ciclo de ejecuci√≥n, el manejo de estado y la orquestaci√≥n, que antes ten√≠amos que programar manualmente.\n",
    "\n",
    "En el siguiente notebook, exploraremos **CrewAI**, otro framework de alto nivel que se especializa en la creaci√≥n de equipos de agentes que colaboran para resolver tareas a√∫n m√°s complejas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
