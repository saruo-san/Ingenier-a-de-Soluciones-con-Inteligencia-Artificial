{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Agentes con Memoria Conversacional en LangChain\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender por qué la memoria es crucial para crear agentes conversacionales efectivos.\n",
    "- Aprender a gestionar el historial de una conversación (`chat_history`) con el `AgentExecutor`.\n",
    "- Implementar un agente que recuerde interacciones pasadas para responder preguntas de seguimiento.\n",
    "- Entender cómo LangChain pasa el contexto de la conversación al LLM.\n",
    "\n",
    "## ¿Qué es la Memoria y Por Qué es Importante?\n",
    "\n",
    "Por defecto, los LLMs y los agentes que hemos construido hasta ahora **no tienen estado (stateless)**. Cada vez que los invocamos, procesan la solicitud como si fuera la primera vez que interactúan con nosotros. No tienen recuerdo de preguntas o respuestas anteriores.\n",
    "\n",
    "Esto es una gran limitación para crear asistentes o chatbots útiles. Un usuario espera poder hacer preguntas de seguimiento, referirse a información mencionada previamente y tener una conversación fluida. \n",
    "\n",
    "La **memoria** es el mecanismo que permite a un agente recordar interacciones pasadas. LangChain facilita enormemente la gestión de esta memoria. La forma más común de memoria es el **historial de chat (chat history)**, donde simplemente guardamos la lista de todos los mensajes de la conversación.\n",
    "\n",
    "En este notebook, veremos cómo añadir esta capacidad a nuestro agente de LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instalación y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai openai wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM de LangChain configurado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wikipedia\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Configurar el idioma de Wikipedia\n",
    "wikipedia.set_lang(\"es\")\n",
    "\n",
    "# Configuración del LLM\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        openai_api_base=os.environ.get(\"GITHUB_BASE_URL\"),\n",
    "        openai_api_key=os.environ.get(\"GITHUB_TOKEN\"),\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"✅ LLM de LangChain configurado.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error configurando el LLM: {e}\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Herramientas y Agente (Sin Cambios)\n",
    "\n",
    "La definición de las herramientas y la creación del agente son exactamente las mismas que en el notebook anterior. La magia de la memoria no está en la definición del agente, sino en **cómo lo ejecutamos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agente y herramientas listos.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool, create_openai_tools_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "@tool\n",
    "def get_wikipedia_summary(query: str) -> str:\n",
    "    \"\"\"Busca en Wikipedia un tema y devuelve un resumen de 2 frases. Útil para obtener información sobre personas, lugares o conceptos.\"\"\"\n",
    "    try:\n",
    "        return wikipedia.summary(query, sentences=2)\n",
    "    except Exception as e:\n",
    "        return f\"Ocurrió un error: {e}\"\n",
    "\n",
    "tools = [get_wikipedia_summary]\n",
    "\n",
    "# Usamos el mismo prompt de la comunidad que ya está preparado para manejar historial\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "print(\"✅ Agente y herramientas listos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gestionando la Memoria Conversacional\n",
    "\n",
    "Para que el agente recuerde, necesitamos hacer dos cosas:\n",
    "\n",
    "1.  **Mantener un historial**: Crearemos una lista llamada `chat_history` para almacenar los mensajes.\n",
    "2.  **Pasar el historial en cada llamada**: El `AgentExecutor` acepta un parámetro `chat_history`. LangChain se encarga de formatear esta lista y añadirla al prompt que se envía al LLM.\n",
    "\n",
    "El formato del historial es una lista de objetos `BaseMessage` de LangChain. Los más comunes son `HumanMessage` (para el usuario) y `AIMessage` (para la respuesta del agente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Iniciamos el historial de chat como una lista vacía\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primera Interacción: Sin Historial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_wikipedia_summary` with `{'query': 'Saturno (planeta)'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSaturno es el sexto planeta del sistema solar contando desde el Sol, el segundo en tamaño y masa después de Júpiter y el único con un sistema de anillos visible desde la Tierra. Su nombre proviene del dios romano Saturno.\u001b[0m\u001b[32;1m\u001b[1;3mSaturno es el sexto planeta del sistema solar, conocido por ser el segundo en tamaño y masa después de Júpiter. Es famoso por su sistema de anillos, que es visible desde la Tierra, y su nombre proviene del dios romano Saturno.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta 1: Saturno es el sexto planeta del sistema solar, conocido por ser el segundo en tamaño y masa después de Júpiter. Es famoso por su sistema de anillos, que es visible desde la Tierra, y su nombre proviene del dios romano Saturno.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"Háblame del planeta Saturno\"\n",
    "\n",
    "response1 = agent_executor.invoke({\n",
    "    \"input\": query1,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(f\"Respuesta 1: {response1['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, actualizamos manualmente nuestro historial con la pregunta del usuario y la respuesta del agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historial actualizado.\n"
     ]
    }
   ],
   "source": [
    "chat_history.append(HumanMessage(content=query1))\n",
    "chat_history.append(AIMessage(content=response1[\"output\"]))\n",
    "\n",
    "print(\"Historial actualizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segunda Interacción: Con Historial\n",
    "\n",
    "Ahora hacemos una pregunta de seguimiento. Fíjate que no mencionamos \"Saturno\", simplemente preguntamos \"¿de qué están hechos sus anillos?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mLos anillos de Saturno están compuestos principalmente de partículas de hielo de agua, con pequeñas cantidades de roca y polvo. Estas partículas varían en tamaño, desde diminutos granos de polvo hasta fragmentos tan grandes como una casa. Los anillos son extremadamente delgados en comparación con su extensión, lo que los hace una de las características más fascinantes del sistema solar.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta 2: Los anillos de Saturno están compuestos principalmente de partículas de hielo de agua, con pequeñas cantidades de roca y polvo. Estas partículas varían en tamaño, desde diminutos granos de polvo hasta fragmentos tan grandes como una casa. Los anillos son extremadamente delgados en comparación con su extensión, lo que los hace una de las características más fascinantes del sistema solar.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"¿Y de qué están hechos sus anillos?\"\n",
    "\n",
    "response2 = agent_executor.invoke({\n",
    "    \"input\": query2,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(f\"Respuesta 2: {response2['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Funcionó! El agente entendió que \"sus anillos\" se refería a los anillos de Saturno, porque la conversación anterior estaba en su contexto. El `verbose=True` nos muestra que el agente decidió buscar en Wikipedia \"anillos de Saturno\", combinando la nueva pregunta con el historial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Añadir memoria a un agente de LangChain es sorprendentemente sencillo, pero increíblemente poderoso. Simplemente manteniendo una lista del historial de chat y pasándola en cada invocación, transformamos un agente de una sola respuesta en un verdadero **asistente conversacional**.\n",
    "\n",
    "La clave es que los componentes de LangChain (`AgentExecutor`, los prompts de `hub`) ya están diseñados para buscar y utilizar la variable `chat_history` si se proporciona.\n",
    "\n",
    "**Limitaciones:**\n",
    "- **Gestión Manual**: En este ejemplo, actualizamos la lista `chat_history` manualmente. Para una aplicación real, querríamos encapsular esto en una clase o función.\n",
    "- **Tamaño del Contexto**: Enviar el historial completo en cada llamada puede volverse costoso y exceder el límite de tokens del modelo en conversaciones muy largas.\n",
    "\n",
    "En los próximos notebooks, exploraremos los **sistemas de memoria** que LangChain ofrece para gestionar estas limitaciones, como la memoria de búfer (`BufferMemory`) que automatiza la gestión del historial y la memoria de resumen (`SummaryMemory`) que condensa conversaciones largas para ahorrar tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
