{
 "cells": [
  {
   "cell_type": "code",
   "id": "00faf4fb",
   "metadata": {},
   "outputs": [],
   "source": "# Configuraci√≥n del modelo con streaming habilitado\ntry:\n    llm = ChatOpenAI(\n        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n        api_key=os.getenv(\"GITHUB_TOKEN\"),\n        model=\"gpt-4o\",\n        streaming=True,  # ¬°Importante: habilitar streaming!\n        temperature=0.7\n    )\n    \n    print(\"‚úì Modelo configurado con streaming habilitado\")\n    print(f\"Modelo: {llm.model_name}\")\n    print(f\"Streaming: {llm.streaming}\")\n    \nexcept Exception as e:\n    print(f\"‚úó Error en configuraci√≥n: {e}\")\n    print(\"Verifica las variables de entorno\")"
  },
  {
   "cell_type": "markdown",
   "id": "mdie8torny",
   "source": "## Consideraciones T√©cnicas del Streaming\n\n### Cu√°ndo Usar Streaming:\n‚úÖ **S√ç usar streaming:**\n- Respuestas largas (>100 tokens)\n- Aplicaciones interactivas\n- Chatbots y asistentes\n- Demostraciones en vivo\n- Cuando la UX es prioritaria\n\n‚ùå **NO usar streaming:**\n- Respuestas muy cortas\n- Procesamiento batch\n- APIs de backend sin interfaz\n- Cuando necesitas la respuesta completa antes de procesar\n\n### Mejores Pr√°cticas:\n1. **Manejo de errores**: Siempre incluye try/catch\n2. **Indicadores visuales**: Muestra progreso al usuario\n3. **Cancelaci√≥n**: Permite al usuario interrumpir\n4. **Buffer management**: Para interfaces web, considera buffering\n5. **Performance**: Monitorea el uso de recursos\n\n## Ejercicios Pr√°cticos\n\n### Ejercicio 1: Indicador de Progreso\nModifica el c√≥digo para mostrar un indicador de progreso (spinner, barra, porcentaje).\n\n### Ejercicio 2: Streaming con Filtros\nImplementa streaming que filtre o procese chunks espec√≠ficos (ej: resaltar palabras clave).\n\n### Ejercicio 3: Chatbot Mejorado\nExtiende el chatbot con:\n- Historial de conversaci√≥n\n- Comandos especiales (/help, /clear)\n- Diferentes personalidades\n\n## Conceptos Clave Aprendidos\n\n1. **Streaming** mejora la percepci√≥n de velocidad\n2. **Chunks** se procesan individualmente en tiempo real\n3. **UX** es significativamente mejor con streaming\n4. **Implementaci√≥n** requiere manejo cuidadoso de generadores\n5. **Casos de uso** espec√≠ficos donde streaming aporta valor\n\n## Pr√≥ximos Pasos\n\nEn el siguiente notebook exploraremos la **memoria en LangChain**, que nos permite mantener contexto entre m√∫ltiples interacciones del usuario.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "18tqodvfkuw",
   "source": "# Chatbot simple con streaming\ndef chatbot_streaming():\n    print(\"=== CHATBOT CON STREAMING ===\")\n    print(\"Escribe 'salir' para terminar la conversaci√≥n\\\\n\")\n    \n    # Configurar asistente con personalidad\n    system_message = \"\"\"Eres un asistente √∫til y amigable especializado en tecnolog√≠a. \n    Respondes de manera clara y concisa, y siempre intentas ser educativo.\"\"\"\n    \n    while True:\n        # Obtener input del usuario\n        user_input = input(\"\\\\nüßë T√∫: \")\n        \n        if user_input.lower() in ['salir', 'exit', 'quit']:\n            print(\"\\\\nüëã ¬°Hasta luego!\")\n            break\n            \n        if not user_input.strip():\n            continue\n            \n        print(\"\\\\nü§ñ Asistente: \", end=\"\", flush=True)\n        \n        try:\n            # Streaming de la respuesta\n            messages = [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_input}\n            ]\n            \n            # Convertir a formato LangChain\n            from langchain.schema import SystemMessage\n            lc_messages = [\n                SystemMessage(content=system_message),\n                HumanMessage(content=user_input)\n            ]\n            \n            full_response = \"\"\n            for chunk in llm.stream(lc_messages):\n                content = chunk.content\n                print(content, end=\"\", flush=True)\n                full_response += content\n                time.sleep(0.02)\n                \n            print()  # Nueva l√≠nea al final\n            \n        except KeyboardInterrupt:\n            print(\"\\\\n\\\\n‚è∏Ô∏è Interrumpido por el usuario\")\n            break\n        except Exception as e:\n            print(f\"\\\\n‚ùå Error: {e}\")\n            \n    print(\"\\\\n¬°Gracias por usar el chatbot!\")\n\n# Ejecutar chatbot (¬°Pru√©balo!)\n# chatbot_streaming()  # Descomenta esta l√≠nea para ejecutar\n\nprint(\"üí° Descomenta la l√≠nea anterior para probar el chatbot interactivo\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "h446kt46h6",
   "source": "## Implementaci√≥n de un Chatbot Simple con Streaming\n\nCreemos un chatbot b√°sico que demuestre el streaming en un contexto pr√°ctico.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7vq3cxittnn",
   "source": "# Comparaci√≥n entre streaming y no-streaming\ndef comparar_streaming():\n    # Modelo sin streaming\n    llm_no_stream = ChatOpenAI(\n        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n        api_key=os.getenv(\"GITHUB_TOKEN\"),\n        model=\"gpt-4o\",\n        streaming=False,  # Sin streaming\n        temperature=0.7\n    )\n    \n    prompt = \"Escribe un p√°rrafo sobre las ventajas de la programaci√≥n en Python\"\n    \n    print(\"=== COMPARACI√ìN: STREAMING vs NO-STREAMING ===\\\\n\")\n    \n    # 1. Sin streaming\n    print(\"1. SIN STREAMING:\")\n    print(\"-\" * 20)\n    print(\"Esperando respuesta completa...\")\n    \n    start_time = time.time()\n    try:\n        response = llm_no_stream.invoke([HumanMessage(content=prompt)])\n        end_time = time.time()\n        \n        print(f\"\\\\n[Respuesta recibida despu√©s de {end_time - start_time:.2f} segundos]\")\n        print(response.content)\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n    \n    print(\"\\\\n\" + \"=\"*60 + \"\\\\n\")\n    \n    # 2. Con streaming\n    print(\"2. CON STREAMING:\")\n    print(\"-\" * 18)\n    print(\"Respuesta en tiempo real:\")\n    \n    start_time = time.time()\n    try:\n        for chunk in llm.stream([HumanMessage(content=prompt)]):\n            print(chunk.content, end=\"\", flush=True)\n            time.sleep(0.03)  # Simular pausa para efecto visual\n        \n        end_time = time.time()\n        print(f\"\\\\n\\\\n[Streaming completado en {end_time - start_time:.2f} segundos]\")\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n    \n    print(\"\\\\n\" + \"=\"*60)\n    print(\"OBSERVACIONES:\")\n    print(\"- Sin streaming: El usuario espera sin feedback\")\n    print(\"- Con streaming: El usuario ve progreso inmediato\")\n    print(\"- Mejor percepci√≥n de velocidad con streaming\")\n    print(\"- Streaming es especial para respuestas largas\")\n\n# Ejecutar comparaci√≥n\ncomparar_streaming()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1g82sf8rfc4",
   "source": "## Comparaci√≥n: Streaming vs No-Streaming\n\nVeamos la diferencia en experiencia de usuario entre ambos enfoques.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ojwqtoiq39",
   "source": "# Streaming con an√°lisis de chunks\ndef streaming_avanzado():\n    prompt = \"Explica qu√© es la inteligencia artificial y c√≥mo funciona el machine learning\"\n    \n    print(\"=== STREAMING AVANZADO CON AN√ÅLISIS ===\")\n    print(\"Analizando chunks conforme llegan...\\n\")\n    \n    # Variables para estad√≠sticas\n    chunk_count = 0\n    total_content = \"\"\n    words_processed = 0\n    \n    try:\n        for chunk in llm.stream([HumanMessage(content=prompt)]):\n            chunk_count += 1\n            content = chunk.content\n            total_content += content\n            \n            # Contar palabras aproximadas\n            if content.strip():\n                words_in_chunk = len(content.split())\n                words_processed += words_in_chunk\n            \n            # Mostrar progreso cada 10 chunks\n            if chunk_count % 10 == 0:\n                print(f\"\\\\n[Progreso: {chunk_count} chunks, ~{words_processed} palabras]\\\\n\")\n            \n            # Imprimir el contenido\n            print(content, end=\"\", flush=True)\n            time.sleep(0.02)  # Pausa ligeramente m√°s larga para ver el an√°lisis\n        \n        # Estad√≠sticas finales\n        print(f\"\\\\n\\\\n=== ESTAD√çSTICAS FINALES ===\")\n        print(f\"Total de chunks: {chunk_count}\")\n        print(f\"Palabras aproximadas: {words_processed}\")\n        print(f\"Caracteres totales: {len(total_content)}\")\n        print(f\"Promedio chars/chunk: {len(total_content)/chunk_count if chunk_count > 0 else 0:.1f}\")\n        \n    except Exception as e:\n        print(f\"\\\\n‚úó Error: {e}\")\n\n# Ejecutar streaming avanzado\nstreaming_avanzado()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "faam63fkjs7",
   "source": "## Streaming Avanzado con Manejo de Chunks\n\nPodemos procesar cada chunk individualmente para crear experiencias m√°s sofisticadas.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ojvw4yr2au",
   "source": "# Ejemplo b√°sico de streaming\ndef streaming_basico():\n    prompt = \"Cu√©ntame una historia corta sobre un programador que descubre la magia en el c√≥digo\"\n    \n    print(\"=== STREAMING EN TIEMPO REAL ===\")\n    print(\"Generando respuesta...\")\n    print(\"-\" * 50)\n    \n    try:\n        # stream() devuelve un generador de chunks\n        for chunk in llm.stream([HumanMessage(content=prompt)]):\n            # Imprimir cada chunk sin nueva l√≠nea\n            print(chunk.content, end=\"\", flush=True)\n            time.sleep(0.01)  # Peque√±a pausa para simular streaming visual\n            \n        print(\"\\n\" + \"-\" * 50)\n        print(\"‚úì Streaming completado\")\n        \n    except Exception as e:\n        print(f\"‚úó Error en streaming: {e}\")\n\n# Ejecutar streaming b√°sico\nstreaming_basico()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "k6fjpd7sg4",
   "source": "## Streaming B√°sico\n\nEl m√©todo `.stream()` devuelve un generador que produce chunks de texto conforme se generan.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "otqx6nclirj",
   "source": "# Importar bibliotecas necesarias\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage\nimport os\nimport time\n\nprint(\"Bibliotecas importadas correctamente para streaming\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hfn0f7nyumn",
   "source": "# 3. LangChain Streaming - Respuestas en Tiempo Real\n\n## Objetivos de Aprendizaje\n- Comprender qu√© es el streaming y cu√°ndo usarlo\n- Implementar streaming con LangChain\n- Manejar chunks de datos en tiempo real\n- Construir interfaces de usuario reactivas\n\n## ¬øQu√© es el Streaming?\n\nEl streaming permite recibir la respuesta del modelo **token por token** conforme se genera, en lugar de esperar a que termine completamente. Esto mejora significativamente la experiencia de usuario en aplicaciones interactivas.\n\n### Ventajas del Streaming:\n- **Percepci√≥n de velocidad**: El usuario ve progreso inmediato\n- **Mejor UX**: Interfaces m√°s reactivas e interactivas  \n- **Engagement**: Mantiene la atenci√≥n del usuario\n- **Debugging**: Permite ver el proceso de generaci√≥n\n\n### Casos de Uso Ideales:\n- Chatbots y asistentes conversacionales\n- Generaci√≥n de contenido largo\n- Aplicaciones web interactivas\n- Demostraciones en vivo",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}