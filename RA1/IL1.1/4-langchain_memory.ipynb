{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bmsvkxtyalo",
   "metadata": {},
   "source": [
    "# 4. LangChain Memory - Gesti√≥n de Contexto Conversacional\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender la importancia de la memoria en conversaciones con LLMs\n",
    "- Implementar diferentes tipos de memoria con LangChain\n",
    "- Gestionar el contexto de conversaciones largas\n",
    "- Optimizar el uso de tokens con estrategias de memoria\n",
    "\n",
    "## ¬øPor qu√© es Importante la Memoria?\n",
    "\n",
    "Los LLMs son **stateless** por naturaleza: no recuerdan conversaciones anteriores. La memoria permite:\n",
    "- **Contexto conversacional**: Referirse a mensajes anteriores\n",
    "- **Personalizaci√≥n**: Recordar preferencias del usuario\n",
    "- **Continuidad**: Mantener hilos de conversaci√≥n coherentes\n",
    "- **Experiencia natural**: Conversaciones que se sienten humanas\n",
    "\n",
    "## Tipos de Memoria en LangChain\n",
    "\n",
    "1. **ConversationBufferMemory**: Mantiene todo el historial\n",
    "2. **ConversationSummaryMemory**: Resume conversaciones largas\n",
    "3. **ConversationBufferWindowMemory**: Mantiene solo los N mensajes m√°s recientes\n",
    "4. **ConversationSummaryBufferMemory**: Combina resumen + buffer reciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias para memoria\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    ConversationBufferWindowMemory\n",
    ")\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"‚úì Bibliotecas de memoria importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del modelo para memoria\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Modelo configurado para experimentos de memoria\")\n",
    "    print(f\"Modelo: {llm.model_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error en configuraci√≥n: {e}\")\n",
    "    print(\"Verifica las variables de entorno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt con historial + entrada del usuario\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente √∫til.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Cadena = prompt + modelo\n",
    "chain = prompt | llm\n",
    "\n",
    "# Almac√©n de historiales\n",
    "store = {}\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Envolver con memoria\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc98272",
   "metadata": {},
   "source": [
    "## 1. ConversationBufferMemory - Memoria Completa\n",
    "\n",
    "Esta memoria mantiene **todo** el historial de la conversaci√≥n. Es la m√°s simple pero puede consumir muchos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb2837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo b√°sico con RunnableWithMessageHistory\n",
    "\n",
    "# Prompt con historial + entrada\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente √∫til.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Cadena = prompt + modelo\n",
    "chain = prompt | llm\n",
    "\n",
    "# Almac√©n de memorias por sesi√≥n\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    \"\"\"Devuelve (o crea) el historial completo para la sesi√≥n.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Envolver con RunnableWithMessageHistory\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "def ejemplo_buffer_memory():\n",
    "    print(\"=== CONVERSATIONBUFFERMEMORY ===\")\n",
    "    print(\"Mantiene todo el historial de conversaci√≥n\\n\")\n",
    "    \n",
    "    session_id = \"demo_session\"\n",
    "\n",
    "    try:\n",
    "        # Primera interacci√≥n\n",
    "        print(\"1. Primera pregunta:\")\n",
    "        response1 = conversation.invoke(\n",
    "            {\"input\": \"Mi nombre es Ana y soy programadora Python\"},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        print(f\"Respuesta: {response1.content}\\n\")\n",
    "\n",
    "        # Segunda interacci√≥n\n",
    "        print(\"2. Segunda pregunta:\")\n",
    "        response2 = conversation.invoke(\n",
    "            {\"input\": \"¬øCu√°l es mi nombre y profesi√≥n?\"},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        print(f\"Respuesta: {response2.content}\\n\")\n",
    "\n",
    "        # Tercera interacci√≥n\n",
    "        print(\"3. Tercera pregunta:\")\n",
    "        response3 = conversation.invoke(\n",
    "            {\"input\": \"¬øQu√© lenguaje de programaci√≥n mencion√©?\"},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        print(f\"Respuesta: {response3.content}\\n\")\n",
    "\n",
    "        # Mostrar historial\n",
    "        print(\"=== CONTENIDO DE LA MEMORIA ===\")\n",
    "        history = store[session_id].messages\n",
    "        for i, msg in enumerate(history, 1):\n",
    "            print(f\"{i}. {msg.type}: {msg.content}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar\n",
    "ejemplo_buffer_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1acba3",
   "metadata": {},
   "source": [
    "## 2. ConversationBufferWindowMemory - Ventana Deslizante\n",
    "\n",
    "Esta memoria mantiene solo los **N mensajes m√°s recientes**, √∫til para controlar el uso de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46248c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt con historial + entrada\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente √∫til.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Cadena = prompt + modelo\n",
    "chain = prompt | llm\n",
    "\n",
    "# Almac√©n de memorias por sesi√≥n\n",
    "store = {}\n",
    "\n",
    "class WindowChatMessageHistory(BaseChatMessageHistory):\n",
    "    \"\"\"Historial de chat que mantiene solo los √∫ltimos k intercambios.\"\"\"\n",
    "    \n",
    "    def __init__(self, k: int = 2):\n",
    "        self.k = k\n",
    "        self._messages = []\n",
    "    \n",
    "    @property\n",
    "    def messages(self):\n",
    "        # Mantener solo los √∫ltimos k intercambios (k*2 mensajes: user + assistant)\n",
    "        return self._messages[-(self.k * 2):]\n",
    "    \n",
    "    def add_message(self, message):\n",
    "        self._messages.append(message)\n",
    "    \n",
    "    def clear(self):\n",
    "        self._messages.clear()\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"Devuelve el historial de ventana para la sesi√≥n.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = WindowChatMessageHistory(k=2)\n",
    "    return store[session_id]\n",
    "\n",
    "# Envolver con RunnableWithMessageHistory\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# Ejemplo\n",
    "def ejemplo_window_memory():\n",
    "    print(\"=== CONVERSATION BUFFER WINDOW MEMORY (k=2) ===\")\n",
    "    print(\"Mantiene solo los 2 intercambios m√°s recientes\\n\")\n",
    "    \n",
    "    session_id = \"demo_window\"\n",
    "    inputs = [\n",
    "        \"Mi nombre es Carlos y tengo 30 a√±os\",\n",
    "        \"Trabajo como dise√±ador gr√°fico\", \n",
    "        \"Me gusta el caf√© y la m√∫sica jazz\",\n",
    "        \"¬øPuedes recordar mi edad?\",\n",
    "        \"¬øCu√°l es mi profesi√≥n?\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for i, user_input in enumerate(inputs, 1):\n",
    "            print(f\"{'='*20} INTERACCI√ìN {i} {'='*20}\")\n",
    "            print(f\"üë§ Usuario: {user_input}\")\n",
    "            \n",
    "            response = conversation.invoke(\n",
    "                {\"input\": user_input},\n",
    "                config={\"configurable\": {\"session_id\": session_id}}\n",
    "            )\n",
    "            print(f\"ü§ñ Asistente: {response.content}\\n\")\n",
    "            \n",
    "            # Obtener el historial\n",
    "            history = get_session_history(session_id)\n",
    "            \n",
    "            # Mostrar comparaci√≥n clara\n",
    "            total_messages = len(history._messages)\n",
    "            visible_messages = len(history.messages)\n",
    "            \n",
    "            print(f\"üìä ESTADO DE LA MEMORIA:\")\n",
    "            print(f\"   üíæ Total almacenado: {total_messages} mensajes\")\n",
    "            print(f\"   üëÅÔ∏è  Visible al modelo: {visible_messages} mensajes\")\n",
    "            print(f\"   üóëÔ∏è  Mensajes descartados: {total_messages - visible_messages}\")\n",
    "            \n",
    "            # Mensajes almacenados totalmente\n",
    "            print(f\"\\nüìö HISTORIAL COMPLETO ALMACENADO ({total_messages} mensajes):\")\n",
    "            if total_messages == 0:\n",
    "                print(\"     (Ning√∫n mensaje a√∫n)\")\n",
    "            else:\n",
    "                for j, msg in enumerate(history._messages, 1):\n",
    "                    role = \"üë§ Usuario\" if msg.type == \"human\" else \"ü§ñ Asistente\"\n",
    "                    content = msg.content[:60] + \"...\" if len(msg.content) > 60 else msg.content\n",
    "                    # Marcar si est√° en la ventana visible\n",
    "                    is_visible = j > total_messages - visible_messages\n",
    "                    marker = \"‚úÖ\" if is_visible else \"‚ùå\"\n",
    "                    print(f\"     {j}. {marker} {role}: {content}\")\n",
    "            \n",
    "            # Lo que ve el modelo\n",
    "            print(f\"\\nüîç VENTANA VISIBLE AL MODELO ({visible_messages} mensajes):\")\n",
    "            if visible_messages == 0:\n",
    "                print(\"     (Ning√∫n mensaje visible)\")\n",
    "            else:\n",
    "                for j, msg in enumerate(history.messages, 1):\n",
    "                    role = \"üë§ Usuario\" if msg.type == \"human\" else \"ü§ñ Asistente\"\n",
    "                    content = msg.content[:60] + \"...\" if len(msg.content) > 60 else msg.content\n",
    "                    print(f\"     {j}. ‚úÖ {role}: {content}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar\n",
    "ejemplo_window_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2195b2",
   "metadata": {},
   "source": [
    "## 3. ConversationSummaryMemory - Resumen Inteligente\n",
    "\n",
    "Esta memoria **resume** conversaciones largas en lugar de mantener todo el texto completo, ahorrando tokens significativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Funci√≥n para resumir autom√°ticamente cuando hay muchos mensajes\n",
    "def auto_summarize(session_id: str, max_messages=6):\n",
    "    history = get_session_history(session_id)\n",
    "    \n",
    "    if len(history.messages) > max_messages:\n",
    "        # Mensajes a resumir (todos excepto los √∫ltimos 2)\n",
    "        messages_to_summarize = history.messages[:-2]\n",
    "        \n",
    "        # Crear texto para resumir\n",
    "        conversation_text = \"\"\n",
    "        for msg in messages_to_summarize:\n",
    "            role = \"Usuario\" if msg.type == \"human\" else \"Asistente\"\n",
    "            conversation_text += f\"{role}: {msg.content}\\n\"\n",
    "        \n",
    "        # Generar resumen\n",
    "        summary_response = llm.invoke(f\"Resume esta conversaci√≥n en 2-3 l√≠neas:\\n{conversation_text}\")\n",
    "        summary = summary_response.content\n",
    "        \n",
    "        # Reemplazar mensajes antiguos con el resumen\n",
    "        recent_messages = history.messages[-2:]\n",
    "        history.clear()\n",
    "        history.add_ai_message(f\"[RESUMEN]: {summary}\")\n",
    "        history.messages.extend(recent_messages)\n",
    "\n",
    "# Crear conversaci√≥n\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente √∫til.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    prompt | llm,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "def ejemplo_summary_memory():\n",
    "    print(\"=== CONVERSATION SUMMARY MEMORY ===\")\n",
    "    print(\"Resume conversaciones largas para ahorrar tokens\\n\")\n",
    "    \n",
    "    session_id = \"summary_session\"\n",
    "    \n",
    "    # Conversaci√≥n de ejemplo\n",
    "    inputs = [\n",
    "        \"Hola, soy Mar√≠a Gonz√°lez, ingeniera de software de 35 a√±os\",\n",
    "        \"Trabajo en una startup de fintech en Madrid desarrollando pagos digitales\",\n",
    "        \"Usamos React, Node.js, Docker y Kubernetes en nuestros proyectos\",\n",
    "        \"Mi mayor desaf√≠o es la latencia en transacciones internacionales\",\n",
    "        \"Tambi√©n trabajo en mejorar la UX de nuestra app m√≥vil\",\n",
    "        \"¬øPuedes resumir qui√©n soy y cu√°les son mis principales desaf√≠os?\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for i, user_input in enumerate(inputs, 1):\n",
    "            print(f\"{'='*15} INTERACCI√ìN {i} {'='*15}\")\n",
    "            print(f\"üë§ Usuario: {user_input}\")\n",
    "            \n",
    "            # Resumir autom√°ticamente si es necesario\n",
    "            auto_summarize(session_id)\n",
    "            \n",
    "            response = conversation.invoke(\n",
    "                {\"input\": user_input},\n",
    "                config={\"configurable\": {\"session_id\": session_id}}\n",
    "            )\n",
    "            print(f\"ü§ñ Asistente: {response.content}\\n\")\n",
    "            \n",
    "            # Mostrar estado de la memoria\n",
    "            history = get_session_history(session_id)\n",
    "            total_messages = len(history.messages)\n",
    "            \n",
    "            print(f\"üìä ESTADO DE LA MEMORIA:\")\n",
    "            print(f\"   üíæ Total mensajes: {total_messages}\")\n",
    "            \n",
    "            # Verificar si hay resumen\n",
    "            has_summary = any(\"[RESUMEN]\" in msg.content for msg in history.messages if hasattr(msg, 'content'))\n",
    "            print(f\"   üìù Tiene resumen: {'‚úÖ S√≠' if has_summary else '‚ùå No'}\")\n",
    "            \n",
    "            print(f\"\\nüí¨ CONTENIDO ACTUAL DE LA MEMORIA:\")\n",
    "            for j, msg in enumerate(history.messages, 1):\n",
    "                role = \"üë§ Usuario\" if msg.type == \"human\" else \"ü§ñ Asistente\"\n",
    "                content = msg.content\n",
    "                \n",
    "                # Destacar si es un resumen\n",
    "                if \"[RESUMEN]\" in content:\n",
    "                    role = \"üìù Resumen\"\n",
    "                    content = content.replace(\"[RESUMEN]: \", \"\")\n",
    "                \n",
    "                # Truncar si es muy largo\n",
    "                if len(content) > 80:\n",
    "                    content = content[:80] + \"...\"\n",
    "                \n",
    "                print(f\"   {j}. {role}: {content}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar\n",
    "ejemplo_summary_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85cfea",
   "metadata": {},
   "source": [
    "## Consideraciones T√©cnicas y Mejores Pr√°cticas\n",
    "\n",
    "### Selecci√≥n del Tipo de Memoria\n",
    "\n",
    "| Tipo | Cu√°ndo Usarlo | Ventajas | Desventajas |\n",
    "|------|---------------|----------|-------------|\n",
    "| **Buffer** | Conversaciones cortas | Contexto completo | Alto consumo de tokens |\n",
    "| **Window** | Contexto reciente importante | Eficiente en tokens | Puede perder informaci√≥n clave |\n",
    "| **Summary** | Conversaciones muy largas | Balance eficiencia/contexto | P√©rdida de detalles espec√≠ficos |\n",
    "\n",
    "### Mejores Pr√°cticas:\n",
    "\n",
    "1. **Gesti√≥n de Tokens**:\n",
    "   - Monitorea el uso de tokens regularmente\n",
    "   - Establece l√≠mites m√°ximos para evitar costos excesivos\n",
    "   - Considera el costo vs. calidad del contexto\n",
    "\n",
    "2. **Selecci√≥n Estrat√©gica**:\n",
    "   - Usa Buffer para sesiones cortas e importantes\n",
    "   - Usa Window para conversaciones con contexto limitado\n",
    "   - Usa Summary para sesiones largas de asistencia\n",
    "\n",
    "3. **Optimizaci√≥n**:\n",
    "   - Limpia memoria peri√≥dicamente si es necesario\n",
    "   - Implementa estrategias h√≠bridas seg√∫n el caso de uso\n",
    "   - Considera almacenamiento persistente para memoria a largo plazo\n",
    "\n",
    "## Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: An√°lisis de Consumo\n",
    "Implementa un sistema que monitoree y reporte el uso de tokens con diferentes tipos de memoria.\n",
    "\n",
    "### Ejercicio 2: Memoria H√≠brida\n",
    "Dise√±a una estrategia que combine multiple tipos de memoria seg√∫n el contexto.\n",
    "\n",
    "### Ejercicio 3: Persistencia\n",
    "Extiende el chatbot para guardar y cargar memoria entre sesiones.\n",
    "\n",
    "## Conceptos Clave Aprendidos\n",
    "\n",
    "1. **Importancia de la memoria** en conversaciones naturales\n",
    "2. **Tipos de memoria** y sus casos de uso espec√≠ficos\n",
    "3. **Balance** entre contexto y eficiencia de tokens\n",
    "4. **Implementaci√≥n pr√°ctica** con LangChain\n",
    "5. **Estrategias de optimizaci√≥n** para diferentes escenarios\n",
    "\n",
    "## Conclusi√≥n del M√≥dulo IL1.1\n",
    "\n",
    "Has completado la introducci√≥n a LLMs y conexiones API. Los conceptos aprendidos:\n",
    "\n",
    "1. **APIs directas** vs **frameworks** como LangChain\n",
    "2. **Streaming** para mejor experiencia de usuario\n",
    "3. **Memoria** para conversaciones contextuales\n",
    "4. **Mejores pr√°cticas** de seguridad y optimizaci√≥n\n",
    "\n",
    "### Pr√≥ximos Pasos\n",
    "En **IL1.2** exploraremos t√©cnicas avanzadas de **prompt engineering** incluyendo zero-shot, few-shot, y chain-of-thought prompting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
