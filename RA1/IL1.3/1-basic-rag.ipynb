{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RAG B√°sico - Implementaci√≥n con Streamlit\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender los componentes fundamentales de un sistema RAG\n",
    "- Implementar una b√∫squeda por palabras clave simple\n",
    "- Generar respuestas basadas en contexto recuperado\n",
    "- Construir una interfaz interactiva con Streamlit\n",
    "\n",
    "## ¬øQu√© es RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "RAG es una arquitectura que combina dos fases para generar respuestas:\n",
    "1. **Retrieval (Recuperaci√≥n)**: Busca y recupera informaci√≥n relevante de una base de conocimiento.\n",
    "2. **Generation (Generaci√≥n)**: Utiliza la informaci√≥n recuperada como contexto para que un LLM genere una respuesta precisa y fundamentada.\n",
    "\n",
    "### Ventajas:\n",
    "- **Reduce alucinaciones**: Las respuestas se basan en datos concretos.\n",
    "- **Conocimiento actualizado**: La base de conocimiento puede actualizarse f√°cilmente.\n",
    "- **Transparencia**: Se puede citar la fuente de la informaci√≥n.\n",
    "\n",
    "### Componentes de nuestro RAG B√°sico:\n",
    "- **Base de documentos**: Una lista de strings en memoria.\n",
    "- **Mecanismo de retrieval**: B√∫squeda simple por coincidencia de palabras clave.\n",
    "- **Generador**: Un LLM (GPT-4o) que recibe el contexto y la pregunta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalaci√≥n de dependencias\n",
    "!pip install openai streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: Importaciones y configuraci√≥n\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cliente OpenAI inicializado\n"
     ]
    }
   ],
   "source": [
    "# Inicializaci√≥n del cliente OpenAI\n",
    "def initialize_client():\n",
    "    client = OpenAI(\n",
    "        base_url=os.getenv(\"GITHUB_BASE_URL\", \"https://models.inference.ai.azure.com\"),\n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\")\n",
    "    )\n",
    "    return client\n",
    "\n",
    "client = initialize_client()\n",
    "print(\"‚úÖ Cliente OpenAI inicializado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de documentos\n",
    "\n",
    "Cargamos la base de documentos que vamos a utilizar para el RAG. En este caso, vamos a utilizar una base de documentos que contiene informaci√≥n sobre la historia de Argentina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Base de documentos creada con 5 documentos:\n",
      "1. La inteligencia artificial es una rama de la inform√°tica que...\n",
      "2. Los modelos de lenguaje grande (LLM) son sistemas de IA entr...\n",
      "3. RAG (Retrieval-Augmented Generation) combina la b√∫squeda de ...\n",
      "4. LangChain es un framework que facilita el desarrollo de apli...\n",
      "5. El prompt engineering es la pr√°ctica de dise√±ar instruccione...\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"La inteligencia artificial es una rama de la inform√°tica que busca crear m√°quinas capaces de realizar tareas que requieren inteligencia humana.\",\n",
    "    \"Los modelos de lenguaje grande (LLM) son sistemas de IA entrenados en enormes cantidades de texto para generar y comprender lenguaje natural.\",\n",
    "    \"RAG (Retrieval-Augmented Generation) combina la b√∫squeda de informaci√≥n relevante con la generaci√≥n de texto para producir respuestas m√°s precisas.\",\n",
    "    \"LangChain es un framework que facilita el desarrollo de aplicaciones con modelos de lenguaje, proporcionando herramientas para cadenas y agentes.\",\n",
    "    \"El prompt engineering es la pr√°ctica de dise√±ar instrucciones efectivas para obtener los mejores resultados de los modelos de IA.\"\n",
    "]\n",
    "\n",
    "print(f\"üìö Base de documentos creada con {len(documents)} documentos:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funci√≥n de recuperaci√≥n simple\n",
    "\n",
    "La funci√≥n simple_retrieval() es una funci√≥n que toma una pregunta y un √≠ndice, y devuelve una lista de documentos relevantes. En este caso, la funci√≥n simplemente devuelve los primeros 3 documentos del √≠ndice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Prueba de recuperaci√≥n para: '¬øQu√© es la inteligencia artificial?'\n",
      "Documentos relevantes encontrados: 3\n",
      "1. La inteligencia artificial es una rama de la inform√°tica que busca crear m√°quinas capaces de realizar tareas que requieren inteligencia humana.\n",
      "2. Los modelos de lenguaje grande (LLM) son sistemas de IA entrenados en enormes cantidades de texto para generar y comprender lenguaje natural.\n",
      "3. RAG (Retrieval-Augmented Generation) combina la b√∫squeda de informaci√≥n relevante con la generaci√≥n de texto para producir respuestas m√°s precisas.\n"
     ]
    }
   ],
   "source": [
    "def simple_retrieval(query, documents):\n",
    "    relevant_docs = []\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    for doc in documents:\n",
    "        if any(word in doc.lower() for word in query_lower.split()):\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    return relevant_docs[:3]\n",
    "\n",
    "# Prueba de la funci√≥n de recuperaci√≥n\n",
    "test_query = \"¬øQu√© es la inteligencia artificial?\"\n",
    "relevant_docs = simple_retrieval(test_query, documents)\n",
    "\n",
    "print(f\"üîç Prueba de recuperaci√≥n para: '{test_query}'\")\n",
    "print(f\"Documentos relevantes encontrados: {len(relevant_docs)}\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"{i}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funci√≥n de generaci√≥n de respuesta\n",
    "La funci√≥n generate_response se encarga de generar una respuesta a partir de una pregunta y un contexto. Para ello, utiliza un modelo de lenguaje preentrenado y una plantilla de prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de generaci√≥n definida\n"
     ]
    }
   ],
   "source": [
    "def generate_response(client, query, context):\n",
    "    prompt = f\"\"\"Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {query}\n",
    "\n",
    "Responde bas√°ndote √∫nicamente en el contexto proporcionado. Si la informaci√≥n no est√° en el contexto, indica que no tienes esa informaci√≥n.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de generaci√≥n definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema RAG completo - Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Pregunta: ¬øQu√© es la inteligencia artificial?\n",
      "\n",
      "==================================================\n",
      "üìã Documentos relevantes (3):\n",
      "1. La inteligencia artificial es una rama de la inform√°tica que busca crear m√°quinas capaces de realizar tareas que requieren inteligencia humana.\n",
      "2. Los modelos de lenguaje grande (LLM) son sistemas de IA entrenados en enormes cantidades de texto para generar y comprender lenguaje natural.\n",
      "3. RAG (Retrieval-Augmented Generation) combina la b√∫squeda de informaci√≥n relevante con la generaci√≥n de texto para producir respuestas m√°s precisas.\n",
      "\n",
      "ü§ñ Respuesta:\n",
      "La inteligencia artificial es una rama de la inform√°tica que busca crear m√°quinas capaces de realizar tareas que requieren inteligencia humana.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"¬øQu√© es la inteligencia artificial?\"\n",
    "\n",
    "print(f\"‚ùì Pregunta: {query1}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Paso 1: Recuperaci√≥n\n",
    "relevant_docs = simple_retrieval(query1, documents)\n",
    "print(f\"üìã Documentos relevantes ({len(relevant_docs)}):\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "\n",
    "# Paso 2: Generaci√≥n\n",
    "if relevant_docs:\n",
    "    context = \"\\n\".join(relevant_docs)\n",
    "    response = generate_response(client, query1, context)\n",
    "    print(f\"\\nü§ñ Respuesta:\")\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron documentos relevantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema RAG completo - Ejemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Pregunta: ¬ø C√≥mo funciona RAG ?\n",
      "\n",
      "==================================================\n",
      "üìã Documentos relevantes (1):\n",
      "1. RAG (Retrieval-Augmented Generation) combina la b√∫squeda de informaci√≥n relevante con la generaci√≥n de texto para producir respuestas m√°s precisas.\n",
      "\n",
      "ü§ñ Respuesta:\n",
      "RAG (Retrieval-Augmented Generation) funciona combinando la b√∫squeda de informaci√≥n relevante con la generaci√≥n de texto para producir respuestas m√°s precisas.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"¬ø C√≥mo funciona RAG ?\"\n",
    "\n",
    "print(f\"‚ùì Pregunta: {query2}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Paso 1: Recuperaci√≥n\n",
    "relevant_docs = simple_retrieval(query2, documents)\n",
    "print(f\"üìã Documentos relevantes ({len(relevant_docs)}):\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "\n",
    "# Paso 2: Generaci√≥n\n",
    "if relevant_docs:\n",
    "    context = \"\\n\".join(relevant_docs)\n",
    "    response = generate_response(client, query2, context)\n",
    "    print(f\"\\nü§ñ Respuesta:\")\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron documentos relevantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema RAG completo - Ejemplo 3 (pregunta sin contexto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Pregunta: ¬øCu√°l es la capital de Francia?\n",
      "\n",
      "==================================================\n",
      "üìã Documentos relevantes (3):\n",
      "1. La inteligencia artificial es una rama de la inform√°tica que busca crear m√°quinas capaces de realizar tareas que requieren inteligencia humana.\n",
      "2. Los modelos de lenguaje grande (LLM) son sistemas de IA entrenados en enormes cantidades de texto para generar y comprender lenguaje natural.\n",
      "3. RAG (Retrieval-Augmented Generation) combina la b√∫squeda de informaci√≥n relevante con la generaci√≥n de texto para producir respuestas m√°s precisas.\n",
      "\n",
      "ü§ñ Respuesta:\n",
      "Seg√∫n el contexto proporcionado, no tengo informaci√≥n sobre cu√°l es la capital de Francia.\n"
     ]
    }
   ],
   "source": [
    "query3 = \"¬øCu√°l es la capital de Francia?\"\n",
    "\n",
    "print(f\"‚ùì Pregunta: {query3}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Paso 1: Recuperaci√≥n\n",
    "relevant_docs = simple_retrieval(query3, documents)\n",
    "print(f\"üìã Documentos relevantes ({len(relevant_docs)}):\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "\n",
    "# Paso 2: Generaci√≥n\n",
    "if relevant_docs:\n",
    "    context = \"\\n\".join(relevant_docs)\n",
    "    response = generate_response(client, query3, context)\n",
    "    print(f\"\\nü§ñ Respuesta:\")\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron documentos relevantes\")\n",
    "    print(\"ü§ñ Respuesta: No tengo informaci√≥n sobre esa pregunta en mi base de documentos.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
